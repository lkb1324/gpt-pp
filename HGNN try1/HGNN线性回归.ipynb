{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49ff8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# 引入线性回归常用的损失函数\n",
    "from torch.nn import Linear, Module, MSELoss\n",
    "import pprint as pp\n",
    "import utils.hypergraph_utils as hgut\n",
    "from models import HGNN\n",
    "from config import get_config\n",
    "from datasets import load_feature_construct_H\n",
    "from datasets import load_ft\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b5b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data\n",
    "data_dir = r\"L:\\HGNN try1\\CG_dataset_rgrs.mat\"\n",
    "fts, lbls, idx_train, idx_test = load_ft(data_dir,'CGNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf762c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5000e+01 3.6440e+03 0.0000e+00 ... 3.3100e+02 4.0500e+02 9.5000e+01]\n",
      " [1.0000e+00 3.4800e+02 0.0000e+00 ... 4.3000e+01 3.2000e+01 1.5000e+01]\n",
      " [1.0000e+00 5.5100e+02 0.0000e+00 ... 5.8000e+01 3.7000e+01 4.0000e+00]\n",
      " ...\n",
      " [8.0000e+00 6.9700e+02 3.0000e+00 ... 4.7000e+01 6.5000e+01 2.2000e+01]\n",
      " [8.0000e+00 9.5000e+02 0.0000e+00 ... 7.8000e+01 1.2200e+02 3.2000e+01]\n",
      " [1.8000e+01 1.9562e+04 0.0000e+00 ... 1.9200e+03 1.6200e+03 4.0000e+02]] [[16  8 33 ...  8  8 18]] [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
      " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
      " 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n",
      " 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n",
      " 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n",
      " 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n",
      " 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n",
      " 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n",
      " 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n",
      " 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n",
      " 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n",
      " 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n",
      " 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n",
      " 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n",
      " 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n",
      " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n",
      " 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n",
      " 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n",
      " 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n",
      " 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n",
      " 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n",
      " 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n",
      " 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n",
      " 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n",
      " 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n",
      " 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n",
      " 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n",
      " 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n",
      " 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n",
      " 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n",
      " 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n",
      " 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n",
      " 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n",
      " 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n",
      " 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n",
      " 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n",
      " 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n",
      " 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n",
      " 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n",
      " 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n",
      " 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n",
      " 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n",
      " 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n",
      " 770. 771. 772. 773. 774. 775. 776. 777. 778. 779. 780. 781. 782. 783.\n",
      " 784. 785. 786. 787. 788. 789. 790. 791. 792. 793. 794. 795. 796. 797.\n",
      " 798. 799. 800. 801. 802. 803. 804. 805. 806. 807. 808. 809. 810. 811.\n",
      " 812. 813. 814. 815. 816. 817. 818. 819. 820. 821. 822. 823. 824. 825.\n",
      " 826. 827. 828. 829. 830. 831. 832. 833. 834. 835. 836. 837. 838. 839.\n",
      " 840. 841. 842. 843. 844. 845. 846. 847. 848. 849. 850. 851. 852.] [ 853.  854.  855.  856.  857.  858.  859.  860.  861.  862.  863.  864.\n",
      "  865.  866.  867.  868.  869.  870.  871.  872.  873.  874.  875.  876.\n",
      "  877.  878.  879.  880.  881.  882.  883.  884.  885.  886.  887.  888.\n",
      "  889.  890.  891.  892.  893.  894.  895.  896.  897.  898.  899.  900.\n",
      "  901.  902.  903.  904.  905.  906.  907.  908.  909.  910.  911.  912.\n",
      "  913.  914.  915.  916.  917.  918.  919.  920.  921.  922.  923.  924.\n",
      "  925.  926.  927.  928.  929.  930.  931.  932.  933.  934.  935.  936.\n",
      "  937.  938.  939.  940.  941.  942.  943.  944.  945.  946.  947.  948.\n",
      "  949.  950.  951.  952.  953.  954.  955.  956.  957.  958.  959.  960.\n",
      "  961.  962.  963.  964.  965.  966.  967.  968.  969.  970.  971.  972.\n",
      "  973.  974.  975.  976.  977.  978.  979.  980.  981.  982.  983.  984.\n",
      "  985.  986.  987.  988.  989.  990.  991.  992.  993.  994.  995.  996.\n",
      "  997.  998.  999. 1000. 1001. 1002. 1003. 1004. 1005. 1006. 1007. 1008.\n",
      " 1009. 1010. 1011. 1012. 1013. 1014. 1015. 1016. 1017. 1018. 1019. 1020.\n",
      " 1021. 1022. 1023. 1024. 1025. 1026. 1027. 1028. 1029. 1030. 1031. 1032.\n",
      " 1033. 1034. 1035. 1036. 1037. 1038. 1039. 1040. 1041. 1042. 1043. 1044.\n",
      " 1045. 1046. 1047. 1048. 1049. 1050. 1051. 1052. 1053. 1054. 1055. 1056.\n",
      " 1057. 1058. 1059. 1060. 1061. 1062. 1063. 1064. 1065.]\n"
     ]
    }
   ],
   "source": [
    "print(fts, lbls, idx_train, idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de543d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_dir\n",
    "H_dir = r\"L:\\hypergraph prediction\\话题用户超图.csv\"\n",
    "W_dir = r\"L:\\hypergraph prediction\\用户超边权值.csv\"\n",
    "H = pd.read_csv(H_dir,header = None)\n",
    "W = pd.read_csv(W_dir)\n",
    "H = H.to_numpy()\n",
    "W = W.iloc[:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95807178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\HGNN try1\\utils\\hypergraph_utils.py:115: RuntimeWarning: divide by zero encountered in power\n",
      "  DV2 = np.mat(np.diag(np.power(DV, -0.5)))\n"
     ]
    }
   ],
   "source": [
    "G = hgut.generate_G_from_H(H,W, variable_weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff8e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = torch.Tensor(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df24a722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1177, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.2500,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0435]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = torch.where(torch.isnan(G), torch.full_like(G, 0), G)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a4eff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fts, lbls, idx_train, idx_test, H = \\\n",
    "    load_feature_construct_H(data_dir,\n",
    "                             m_prob=cfg['m_prob'],\n",
    "                             K_neigs=cfg['K_neigs'],\n",
    "                             is_probH=cfg['is_probH'],\n",
    "                             use_mvcnn_feature=cfg['use_mvcnn_feature'],\n",
    "                             use_gvcnn_feature=cfg['use_gvcnn_feature'],\n",
    "                             use_mvcnn_feature_for_structure=cfg['use_mvcnn_feature_for_structure'],\n",
    "                             use_gvcnn_feature_for_structure=cfg['use_gvcnn_feature_for_structure'])\n",
    "'''\n",
    "\n",
    "# n_class = int(lbls.max()) + 1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # 指定存放的位置 cpu/gpu\n",
    "\n",
    "# transform data to device\n",
    "fts= torch.Tensor(fts).to(device)\n",
    "lbls = torch.Tensor(lbls).squeeze().long().to(device)\n",
    "G = torch.Tensor(G).to(device)\n",
    "idx_train = torch.Tensor(idx_train).long().to(device)\n",
    "idx_test = torch.Tensor(idx_test).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59859c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion,MAE, optimizer, scheduler, num_epochs=25, print_freq=500):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # 分类任务的评估指标acc\n",
    "#     best_acc = 0.0\n",
    "    best_loss = 9999\n",
    "    best_mrse = 9\n",
    "    mae = 999\n",
    "    mrse = 9999\n",
    "    rmse = 999\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 间隔输出训练效果\n",
    "        if epoch % print_freq == 0:\n",
    "            print('-' * 10)\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']: # 每一轮都训一遍再测一遍\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            idx = idx_train if phase == 'train' else idx_test # 选择相应数据集\n",
    "\n",
    "            # Iterate over data.\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(fts, G) # model 输出的是n*1\n",
    "#                 print(outputs[idx].float())\n",
    "                loss = criterion(outputs[idx].float(), lbls[idx].float()) # 根据自定义的criterion计算训练误差\n",
    "                \n",
    "                # _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase 在训练阶段进行反向传播和优化\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * fts.size(0)\n",
    "#             print(fts.size(0))\n",
    "            # running_corrects += torch.sum(preds[idx] == lbls.data[idx])\n",
    "\n",
    "            epoch_loss = running_loss / len(idx)\n",
    "#             epoch_acc = running_corrects.double() / len(idx)\n",
    "\n",
    "            if epoch % print_freq == 0:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "            \n",
    "\n",
    "            # deep copy the model 存储预测效果最好时候的model\n",
    "            # deep copy 复制完整对象并且存储在独立的内存地址\n",
    "            # 在评估阶段选择最好的模型，loss最小\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "#                 best_mrse = mrse\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "   \n",
    "                mae = MAE(outputs[idx].float(), lbls[idx].float())\n",
    "                mrse = MRSE(outputs,lbls,idx).item()\n",
    "                rmse = math.sqrt(best_loss)\n",
    "#                 print(f'mrse: {mrse}')\n",
    "#                 print(f'mae: {mae}')\n",
    "#                 print(f'rmse: {rmse}')\n",
    "\n",
    "        if epoch % print_freq == 0:\n",
    "            print(f'Best val loss: {best_loss:4f}')\n",
    "            print(f'mrse: {mrse}')\n",
    "            print(f'mae: {mae}')\n",
    "            print(f'rmse: {rmse}')\n",
    "           \n",
    "            print('-' * 20)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val loss: {best_loss:4f}')\n",
    "    print(f'mrse: {mrse}')\n",
    "    print(f'mae: {mae}')\n",
    "    print(f'rmse: {rmse}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts) # 读出最好的model并且返回\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc1f71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main():\n",
    "    '''\n",
    "    print(f\"Classification on {cfg['on_dataset']} dataset!!! class number: {n_class}\")\n",
    "    print(f\"use MVCNN feature: {cfg['use_mvcnn_feature']}\")\n",
    "    print(f\"use GVCNN feature: {cfg['use_gvcnn_feature']}\")\n",
    "    print(f\"use MVCNN feature for structure: {cfg['use_mvcnn_feature_for_structure']}\")\n",
    "    print(f\"use GVCNN feature for structure: {cfg['use_gvcnn_feature_for_structure']}\")\n",
    "    print('Configuration -> Start')\n",
    "    pp.pprint(cfg)\n",
    "    print('Configuration -> End')\n",
    "    '''\n",
    "\n",
    "    # 定义模型，此处分类模型被耦合在HGNN函数中，需要去HGNN那里改预测任务\n",
    "    model_ft = HGNN(in_ch=fts.shape[1], # 特征矩阵列数\n",
    "                    n_class=1,\n",
    "                    n_hid=128,\n",
    "                    dropout=0)\n",
    "    model_ft = model_ft.to(device) # 将模型转移到设备上\n",
    "\n",
    "    # adam优化\n",
    "    optimizer = optim.Adam(model_ft.parameters(), lr=0.01,weight_decay=0.005)\n",
    "    # SGD优化\n",
    "#     optimizer = optim.SGD(model_ft.parameters(), lr=0.01, weight_decay=cfg['weight_decay)'])\n",
    "\n",
    "    # 动态调整学习率\n",
    "    # milestone 调整轮次\n",
    "    # gamma 调整倍数\n",
    "    # new_lr = lr*gamma when epoch = milestone[i]\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                               milestones=[100,300,500,800],\n",
    "                                               gamma=0.9)\n",
    "    # criterion = torch.nn.CrossEntropyLoss() # 分类预测任务中使用了交叉熵损失函数\n",
    "    # 线性回归要改成\n",
    "    criterion = torch.nn.MSELoss(reduction = 'mean')\n",
    "    MAE = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "    model_ft = train_model(model_ft, criterion,MAE, optimizer, schedular, 1500, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8e9b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/1499\n",
      "train Loss: 4062.5308\n",
      "val Loss: 653858.4147\n",
      "Best val loss: 9999.000000\n",
      "mrse: 9999\n",
      "mae: 999\n",
      "rmse: 999\n",
      "--------------------\n",
      "----------\n",
      "Epoch 100/1499\n",
      "train Loss: 3597.7955\n",
      "val Loss: 1967.3193\n",
      "Best val loss: 1946.487533\n",
      "mrse: 0.48034271597862244\n",
      "mae: 10.98414421081543\n",
      "rmse: 44.11901555287044\n",
      "--------------------\n",
      "----------\n",
      "Epoch 200/1499\n",
      "train Loss: 3582.7178\n",
      "val Loss: 1936.3456\n",
      "Best val loss: 1935.312958\n",
      "mrse: 0.6953354477882385\n",
      "mae: 12.140671730041504\n",
      "rmse: 43.99219200745056\n",
      "--------------------\n",
      "----------\n",
      "Epoch 300/1499\n",
      "train Loss: 3549.1484\n",
      "val Loss: 1951.8229\n",
      "Best val loss: 1787.845213\n",
      "mrse: 0.4888523519039154\n",
      "mae: 10.692160606384277\n",
      "rmse: 42.282918695508705\n",
      "--------------------\n",
      "----------\n",
      "Epoch 400/1499\n",
      "train Loss: 3514.1502\n",
      "val Loss: 1797.3037\n",
      "Best val loss: 1725.049031\n",
      "mrse: 0.5383505821228027\n",
      "mae: 10.26809024810791\n",
      "rmse: 41.53370957430146\n",
      "--------------------\n",
      "----------\n",
      "Epoch 500/1499\n",
      "train Loss: 3471.5629\n",
      "val Loss: 1776.0195\n",
      "Best val loss: 1674.168323\n",
      "mrse: 0.567175567150116\n",
      "mae: 10.355451583862305\n",
      "rmse: 40.91660204870862\n",
      "--------------------\n",
      "----------\n",
      "Epoch 600/1499\n",
      "train Loss: 3475.4169\n",
      "val Loss: 1791.3299\n",
      "Best val loss: 1661.764262\n",
      "mrse: 0.6058483123779297\n",
      "mae: 10.408085823059082\n",
      "rmse: 40.764742874142605\n",
      "--------------------\n",
      "----------\n",
      "Epoch 700/1499\n",
      "train Loss: 3449.0489\n",
      "val Loss: 1720.6000\n",
      "Best val loss: 1635.297020\n",
      "mrse: 0.6681374311447144\n",
      "mae: 9.796066284179688\n",
      "rmse: 40.43880586401835\n",
      "--------------------\n",
      "----------\n",
      "Epoch 800/1499\n",
      "train Loss: 3440.8248\n",
      "val Loss: 1709.8575\n",
      "Best val loss: 1635.297020\n",
      "mrse: 0.6681374311447144\n",
      "mae: 9.796066284179688\n",
      "rmse: 40.43880586401835\n",
      "--------------------\n",
      "----------\n",
      "Epoch 900/1499\n",
      "train Loss: 3437.5382\n",
      "val Loss: 1690.3082\n",
      "Best val loss: 1635.297020\n",
      "mrse: 0.6681374311447144\n",
      "mae: 9.796066284179688\n",
      "rmse: 40.43880586401835\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1000/1499\n",
      "train Loss: 3438.3263\n",
      "val Loss: 1731.4689\n",
      "Best val loss: 1611.313645\n",
      "mrse: 0.5261454582214355\n",
      "mae: 9.213570594787598\n",
      "rmse: 40.14117144239026\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1100/1499\n",
      "train Loss: 3424.6156\n",
      "val Loss: 1725.6024\n",
      "Best val loss: 1611.313645\n",
      "mrse: 0.5261454582214355\n",
      "mae: 9.213570594787598\n",
      "rmse: 40.14117144239026\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1200/1499\n",
      "train Loss: 3420.0448\n",
      "val Loss: 1706.0276\n",
      "Best val loss: 1611.313645\n",
      "mrse: 0.5261454582214355\n",
      "mae: 9.213570594787598\n",
      "rmse: 40.14117144239026\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1300/1499\n",
      "train Loss: 3416.5596\n",
      "val Loss: 1713.8172\n",
      "Best val loss: 1611.313645\n",
      "mrse: 0.5261454582214355\n",
      "mae: 9.213570594787598\n",
      "rmse: 40.14117144239026\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1400/1499\n",
      "train Loss: 3413.5168\n",
      "val Loss: 1717.1224\n",
      "Best val loss: 1611.313645\n",
      "mrse: 0.5261454582214355\n",
      "mae: 9.213570594787598\n",
      "rmse: 40.14117144239026\n",
      "--------------------\n",
      "\n",
      "Training complete in 1m 5s\n",
      "Best val loss: 1611.313645\n",
      "mrse: 0.5261454582214355\n",
      "mae: 9.213570594787598\n",
      "rmse: 40.14117144239026\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    _main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f3e3d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRSE(outputs,lbls,idx):\n",
    "    sums = 0\n",
    "    count = len(outputs[idx])\n",
    "    for i in range(count):\n",
    "        truth = lbls[idx][i]\n",
    "        pred = outputs[idx][i]\n",
    "        delta =((truth-pred)/truth)*((truth-pred)/truth)\n",
    "        sums = sums+delta\n",
    "    res = sums / count\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3157d69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16,  8, 33,  ...,  8,  8, 18])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f09e9722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.4512)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(lbls.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24195192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46.9552)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(lbls.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202fe6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
