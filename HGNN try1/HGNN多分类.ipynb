{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3eab459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# 引入线性回归常用的损失函数\n",
    "from torch.nn import Linear, Module, MSELoss\n",
    "import pprint as pp\n",
    "import utils.hypergraph_utils as hgut\n",
    "from models import HGNN\n",
    "from config import get_config\n",
    "from datasets import load_feature_construct_H\n",
    "from datasets import load_ft\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df406dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data\n",
    "data_dir = r\"L:\\HGNN try1\\CG_dataset_clsf.mat\"\n",
    "fts, lbls, idx_train, idx_test = load_ft(data_dir,'CGNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0854580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5000e+01 3.6440e+03 0.0000e+00 ... 3.3100e+02 4.0500e+02 9.5000e+01]\n",
      " [1.0000e+00 3.4800e+02 0.0000e+00 ... 4.3000e+01 3.2000e+01 1.5000e+01]\n",
      " [1.0000e+00 5.5100e+02 0.0000e+00 ... 5.8000e+01 3.7000e+01 4.0000e+00]\n",
      " ...\n",
      " [8.0000e+00 6.9700e+02 3.0000e+00 ... 4.7000e+01 6.5000e+01 2.2000e+01]\n",
      " [8.0000e+00 9.5000e+02 0.0000e+00 ... 7.8000e+01 1.2200e+02 3.2000e+01]\n",
      " [1.8000e+01 1.9562e+04 0.0000e+00 ... 1.9200e+03 1.6200e+03 4.0000e+02]] [[3 3 3 ... 3 3 3]] [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      " 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.\n",
      " 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.\n",
      " 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.\n",
      " 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.\n",
      " 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195.\n",
      " 196. 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209.\n",
      " 210. 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223.\n",
      " 224. 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237.\n",
      " 238. 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251.\n",
      " 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265.\n",
      " 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279.\n",
      " 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293.\n",
      " 294. 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307.\n",
      " 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321.\n",
      " 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335.\n",
      " 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349.\n",
      " 350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399. 400. 401. 402. 403. 404. 405.\n",
      " 406. 407. 408. 409. 410. 411. 412. 413. 414. 415. 416. 417. 418. 419.\n",
      " 420. 421. 422. 423. 424. 425. 426. 427. 428. 429. 430. 431. 432. 433.\n",
      " 434. 435. 436. 437. 438. 439. 440. 441. 442. 443. 444. 445. 446. 447.\n",
      " 448. 449. 450. 451. 452. 453. 454. 455. 456. 457. 458. 459. 460. 461.\n",
      " 462. 463. 464. 465. 466. 467. 468. 469. 470. 471. 472. 473. 474. 475.\n",
      " 476. 477. 478. 479. 480. 481. 482. 483. 484. 485. 486. 487. 488. 489.\n",
      " 490. 491. 492. 493. 494. 495. 496. 497. 498. 499. 500. 501. 502. 503.\n",
      " 504. 505. 506. 507. 508. 509. 510. 511. 512. 513. 514. 515. 516. 517.\n",
      " 518. 519. 520. 521. 522. 523. 524. 525. 526. 527. 528. 529. 530. 531.\n",
      " 532. 533. 534. 535. 536. 537. 538. 539. 540. 541. 542. 543. 544. 545.\n",
      " 546. 547. 548. 549. 550. 551. 552. 553. 554. 555. 556. 557. 558. 559.\n",
      " 560. 561. 562. 563. 564. 565. 566. 567. 568. 569. 570. 571. 572. 573.\n",
      " 574. 575. 576. 577. 578. 579. 580. 581. 582. 583. 584. 585. 586. 587.\n",
      " 588. 589. 590. 591. 592. 593. 594. 595. 596. 597. 598. 599. 600. 601.\n",
      " 602. 603. 604. 605. 606. 607. 608. 609. 610. 611. 612. 613. 614. 615.\n",
      " 616. 617. 618. 619. 620. 621. 622. 623. 624. 625. 626. 627. 628. 629.\n",
      " 630. 631. 632. 633. 634. 635. 636. 637. 638. 639. 640. 641. 642. 643.\n",
      " 644. 645. 646. 647. 648. 649. 650. 651. 652. 653. 654. 655. 656. 657.\n",
      " 658. 659. 660. 661. 662. 663. 664. 665. 666. 667. 668. 669. 670. 671.\n",
      " 672. 673. 674. 675. 676. 677. 678. 679. 680. 681. 682. 683. 684. 685.\n",
      " 686. 687. 688. 689. 690. 691. 692. 693. 694. 695. 696. 697. 698. 699.\n",
      " 700. 701. 702. 703. 704. 705. 706. 707. 708. 709. 710. 711. 712. 713.\n",
      " 714. 715. 716. 717. 718. 719. 720. 721. 722. 723. 724. 725. 726. 727.\n",
      " 728. 729. 730. 731. 732. 733. 734. 735. 736. 737. 738. 739. 740. 741.\n",
      " 742. 743. 744. 745. 746. 747. 748. 749. 750. 751. 752. 753. 754. 755.\n",
      " 756. 757. 758. 759. 760. 761. 762. 763. 764. 765. 766. 767. 768. 769.\n",
      " 770. 771. 772. 773. 774. 775. 776. 777. 778. 779. 780. 781. 782. 783.\n",
      " 784. 785. 786. 787. 788. 789. 790. 791. 792. 793. 794. 795. 796. 797.\n",
      " 798. 799. 800. 801. 802. 803. 804. 805. 806. 807. 808. 809. 810. 811.\n",
      " 812. 813. 814. 815. 816. 817. 818. 819. 820. 821. 822. 823. 824. 825.\n",
      " 826. 827. 828. 829. 830. 831. 832. 833. 834. 835. 836. 837. 838. 839.\n",
      " 840. 841. 842. 843. 844. 845. 846. 847. 848. 849. 850. 851. 852.] [ 853.  854.  855.  856.  857.  858.  859.  860.  861.  862.  863.  864.\n",
      "  865.  866.  867.  868.  869.  870.  871.  872.  873.  874.  875.  876.\n",
      "  877.  878.  879.  880.  881.  882.  883.  884.  885.  886.  887.  888.\n",
      "  889.  890.  891.  892.  893.  894.  895.  896.  897.  898.  899.  900.\n",
      "  901.  902.  903.  904.  905.  906.  907.  908.  909.  910.  911.  912.\n",
      "  913.  914.  915.  916.  917.  918.  919.  920.  921.  922.  923.  924.\n",
      "  925.  926.  927.  928.  929.  930.  931.  932.  933.  934.  935.  936.\n",
      "  937.  938.  939.  940.  941.  942.  943.  944.  945.  946.  947.  948.\n",
      "  949.  950.  951.  952.  953.  954.  955.  956.  957.  958.  959.  960.\n",
      "  961.  962.  963.  964.  965.  966.  967.  968.  969.  970.  971.  972.\n",
      "  973.  974.  975.  976.  977.  978.  979.  980.  981.  982.  983.  984.\n",
      "  985.  986.  987.  988.  989.  990.  991.  992.  993.  994.  995.  996.\n",
      "  997.  998.  999. 1000. 1001. 1002. 1003. 1004. 1005. 1006. 1007. 1008.\n",
      " 1009. 1010. 1011. 1012. 1013. 1014. 1015. 1016. 1017. 1018. 1019. 1020.\n",
      " 1021. 1022. 1023. 1024. 1025. 1026. 1027. 1028. 1029. 1030. 1031. 1032.\n",
      " 1033. 1034. 1035. 1036. 1037. 1038. 1039. 1040. 1041. 1042. 1043. 1044.\n",
      " 1045. 1046. 1047. 1048. 1049. 1050. 1051. 1052. 1053. 1054. 1055. 1056.\n",
      " 1057. 1058. 1059. 1060. 1061. 1062. 1063. 1064. 1065.]\n"
     ]
    }
   ],
   "source": [
    "print(fts, lbls, idx_train, idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05b8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_dir\n",
    "H_dir = r\"L:\\hypergraph prediction\\话题用户超图.csv\"\n",
    "W_dir = r\"L:\\hypergraph prediction\\用户超边权值.csv\"\n",
    "H = pd.read_csv(H_dir,header = None)\n",
    "W = pd.read_csv(W_dir)\n",
    "H = H.to_numpy()\n",
    "W = W.iloc[:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77198a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\HGNN try1\\utils\\hypergraph_utils.py:115: RuntimeWarning: divide by zero encountered in power\n",
      "  DV2 = np.mat(np.diag(np.power(DV, -0.5)))\n"
     ]
    }
   ],
   "source": [
    "G = hgut.generate_G_from_H(H,W, variable_weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a2a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = torch.Tensor(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41f485ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1177, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.2500,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0435]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = torch.where(torch.isnan(G), torch.full_like(G, 0), G)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcad0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fts, lbls, idx_train, idx_test, H = \\\n",
    "    load_feature_construct_H(data_dir,\n",
    "                             m_prob=cfg['m_prob'],\n",
    "                             K_neigs=cfg['K_neigs'],\n",
    "                             is_probH=cfg['is_probH'],\n",
    "                             use_mvcnn_feature=cfg['use_mvcnn_feature'],\n",
    "                             use_gvcnn_feature=cfg['use_gvcnn_feature'],\n",
    "                             use_mvcnn_feature_for_structure=cfg['use_mvcnn_feature_for_structure'],\n",
    "                             use_gvcnn_feature_for_structure=cfg['use_gvcnn_feature_for_structure'])\n",
    "'''\n",
    "\n",
    "# n_class = int(lbls.max()) + 1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # 指定存放的位置 cpu/gpu\n",
    "\n",
    "# transform data to device\n",
    "fts= torch.Tensor(fts).to(device)\n",
    "lbls = torch.Tensor(lbls).squeeze().long().to(device)\n",
    "G = torch.Tensor(G).to(device)\n",
    "idx_train = torch.Tensor(idx_train).long().to(device)\n",
    "idx_test = torch.Tensor(idx_test).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a4699d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, print_freq=500):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # 分类任务的评估指标acc\n",
    "    best_acc = 0.0\n",
    "#     best_loss = 555\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 间隔输出训练效果\n",
    "        if epoch % print_freq == 0:\n",
    "            print('-' * 10)\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']: # 每一轮都训一遍再测一遍\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            idx = idx_train if phase == 'train' else idx_test # 选择相应数据集\n",
    "\n",
    "            # Iterate over data.\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(fts, G) # model 输出的是n*1\n",
    "#                 print(outputs)\n",
    "                loss = criterion(outputs[idx], lbls[idx]) # 根据自定义的criterion计算训练误差\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase 在训练阶段进行反向传播和优化\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * fts.size(0)\n",
    "#             print(fts.size(0))\n",
    "            running_corrects += torch.sum(preds[idx] == lbls.data[idx])\n",
    "\n",
    "            epoch_loss = running_loss / len(idx)\n",
    "            epoch_acc = running_corrects.double() / len(idx)\n",
    "\n",
    "            if epoch % print_freq == 0:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc:{epoch_acc:4f}')\n",
    "\n",
    "            # deep copy the model 存储预测效果最好时候的model\n",
    "            # deep copy 复制完整对象并且存储在独立的内存地址\n",
    "            # 在评估阶段选择最好的模型，loss最小\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if epoch % print_freq == 0:\n",
    "            print(f'Best val acc:{best_acc:4f}')\n",
    "            print('-' * 20)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val acc:{best_acc:4f} ')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts) # 读出最好的model并且返回\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "578b4ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main():\n",
    "    '''\n",
    "    print(f\"Classification on {cfg['on_dataset']} dataset!!! class number: {n_class}\")\n",
    "    print(f\"use MVCNN feature: {cfg['use_mvcnn_feature']}\")\n",
    "    print(f\"use GVCNN feature: {cfg['use_gvcnn_feature']}\")\n",
    "    print(f\"use MVCNN feature for structure: {cfg['use_mvcnn_feature_for_structure']}\")\n",
    "    print(f\"use GVCNN feature for structure: {cfg['use_gvcnn_feature_for_structure']}\")\n",
    "    print('Configuration -> Start')\n",
    "    pp.pprint(cfg)\n",
    "    print('Configuration -> End')\n",
    "    '''\n",
    "\n",
    "    # 定义模型，此处分类模型被耦合在HGNN函数中，需要去HGNN那里改预测任务\n",
    "    model_ft = HGNN(in_ch=fts.shape[1], # 特征矩阵列数\n",
    "                    n_class=4,\n",
    "                    n_hid=128,\n",
    "                    dropout=0)\n",
    "    model_ft = model_ft.to(device) # 将模型转移到设备上\n",
    "\n",
    "    # adam优化\n",
    "    optimizer = optim.Adam(model_ft.parameters(), lr=0.01,weight_decay=0.005)\n",
    "    # SGD优化\n",
    "#     optimizer = optim.SGD(model_ft.parameters(), lr=0.01, weight_decay=cfg['weight_decay)'])\n",
    "\n",
    "    # 动态调整学习率\n",
    "    # milestone 调整轮次\n",
    "    # gamma 调整倍数\n",
    "    # new_lr = lr*gamma when epoch = milestone[i]\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                               milestones=[100,300,500,800],\n",
    "                                               gamma=0.9)\n",
    "    criterion = torch.nn.CrossEntropyLoss() # 分类预测任务中使用了交叉熵损失函数\n",
    "    # 线性回归要改成\n",
    "#     criterion = torch.nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "    model_ft = train_model(model_ft, criterion, optimizer, schedular, 1500, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8e9b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/1499\n",
      "train Loss: 8.1433 Acc:0.766706\n",
      "val Loss: 1595.7900 Acc:0.028169\n",
      "Best val acc:0.028169\n",
      "--------------------\n",
      "----------\n",
      "Epoch 100/1499\n",
      "train Loss: 0.6419 Acc:0.889801\n",
      "val Loss: 1.8919 Acc:0.943662\n",
      "Best val acc:0.943662\n",
      "--------------------\n",
      "----------\n",
      "Epoch 200/1499\n",
      "train Loss: 0.5168 Acc:0.896835\n",
      "val Loss: 1.6489 Acc:0.943662\n",
      "Best val acc:0.943662\n",
      "--------------------\n",
      "----------\n",
      "Epoch 300/1499\n",
      "train Loss: 0.5266 Acc:0.890973\n",
      "val Loss: 1.5446 Acc:0.943662\n",
      "Best val acc:0.943662\n",
      "--------------------\n",
      "----------\n",
      "Epoch 400/1499\n",
      "train Loss: 0.4579 Acc:0.893318\n",
      "val Loss: 1.4001 Acc:0.943662\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 500/1499\n",
      "train Loss: 0.4334 Acc:0.908558\n",
      "val Loss: 1.4167 Acc:0.943662\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 600/1499\n",
      "train Loss: 0.4236 Acc:0.907386\n",
      "val Loss: 1.4092 Acc:0.938967\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 700/1499\n",
      "train Loss: 0.4181 Acc:0.910903\n",
      "val Loss: 1.3775 Acc:0.943662\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 800/1499\n",
      "train Loss: 0.4127 Acc:0.908558\n",
      "val Loss: 1.3556 Acc:0.943662\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 900/1499\n",
      "train Loss: 0.4082 Acc:0.909730\n",
      "val Loss: 1.3667 Acc:0.938967\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1000/1499\n",
      "train Loss: 0.4030 Acc:0.910903\n",
      "val Loss: 1.3847 Acc:0.934272\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1100/1499\n",
      "train Loss: 0.5416 Acc:0.887456\n",
      "val Loss: 1.4244 Acc:0.943662\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1200/1499\n",
      "train Loss: 0.4597 Acc:0.896835\n",
      "val Loss: 1.4871 Acc:0.938967\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1300/1499\n",
      "train Loss: 0.4469 Acc:0.900352\n",
      "val Loss: 1.4623 Acc:0.934272\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1400/1499\n",
      "train Loss: 0.4277 Acc:0.902696\n",
      "val Loss: 1.4000 Acc:0.934272\n",
      "Best val acc:0.948357\n",
      "--------------------\n",
      "\n",
      "Training complete in 0m 52s\n",
      "Best val acc:0.948357 \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    _main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hypergraph_utils as hgut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c3806e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\HGNN try1\\utils\\hypergraph_utils.py:115: RuntimeWarning: divide by zero encountered in power\n",
      "  DV2 = np.mat(np.diag(np.power(DV, -0.5)))\n"
     ]
    }
   ],
   "source": [
    "G = hgut.generate_G_from_H(H, W, variable_weight=False)\n",
    "G= torch.Tensor(G)\n",
    "G = torch.where(\n",
    "    torch.isnan(G),\n",
    "    torch.full_like(G, 0),\n",
    "    G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ea90d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# 引入线性回归常用的损失函数\n",
    "from torch.nn import Linear, Module, MSELoss\n",
    "import pprint as pp\n",
    "import utils.hypergraph_utils as hgut\n",
    "from models import HGNN\n",
    "from config import get_config\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7f953dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # 指定存放的位置 cpu/gpu\n",
    "data_dir = r\"L:\\HGNN try1\\CG_dataset.mat\"\n",
    "fts, lbls, idx_train, idx_test = load_ft(data_dir,'CGNN')\n",
    "# transform data to device\n",
    "fts = torch.Tensor(fts).to(device)\n",
    "lbls = torch.Tensor(lbls).squeeze().long().to(device)\n",
    "G = torch.Tensor(G).to(device)\n",
    "idx_train = torch.Tensor(idx_train).long().to(device)\n",
    "idx_test = torch.Tensor(idx_test).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bdc350f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fts = torch.nn.functional.normalize(fts, p=1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "88ce1fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0426e-03, 7.3915e-01, 0.0000e+00,  ..., 6.7140e-02, 8.2150e-02,\n",
      "         1.9270e-02],\n",
      "        [7.7519e-04, 2.6977e-01, 0.0000e+00,  ..., 3.3333e-02, 2.4806e-02,\n",
      "         1.1628e-02],\n",
      "        [1.1534e-03, 6.3552e-01, 0.0000e+00,  ..., 6.6897e-02, 4.2676e-02,\n",
      "         4.6136e-03],\n",
      "        ...,\n",
      "        [4.8485e-03, 4.2242e-01, 1.8182e-03,  ..., 2.8485e-02, 3.9394e-02,\n",
      "         1.3333e-02],\n",
      "        [6.2647e-03, 7.4393e-01, 0.0000e+00,  ..., 6.1081e-02, 9.5536e-02,\n",
      "         2.5059e-02],\n",
      "        [7.6120e-04, 8.2725e-01, 0.0000e+00,  ..., 8.1194e-02, 6.8508e-02,\n",
      "         1.6915e-02]]) tensor([3, 3, 3,  ..., 3, 3, 3]) tensor([[0.1177, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2500,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0435]]) tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "        504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
      "        518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
      "        532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "        546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
      "        560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
      "        574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
      "        588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
      "        602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615,\n",
      "        616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "        630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643,\n",
      "        644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657,\n",
      "        658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671,\n",
      "        672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
      "        686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699,\n",
      "        700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713,\n",
      "        714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
      "        728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n",
      "        742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755,\n",
      "        756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769,\n",
      "        770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783,\n",
      "        784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797,\n",
      "        798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "        812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825,\n",
      "        826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839,\n",
      "        840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852]) tensor([ 853,  854,  855,  856,  857,  858,  859,  860,  861,  862,  863,  864,\n",
      "         865,  866,  867,  868,  869,  870,  871,  872,  873,  874,  875,  876,\n",
      "         877,  878,  879,  880,  881,  882,  883,  884,  885,  886,  887,  888,\n",
      "         889,  890,  891,  892,  893,  894,  895,  896,  897,  898,  899,  900,\n",
      "         901,  902,  903,  904,  905,  906,  907,  908,  909,  910,  911,  912,\n",
      "         913,  914,  915,  916,  917,  918,  919,  920,  921,  922,  923,  924,\n",
      "         925,  926,  927,  928,  929,  930,  931,  932,  933,  934,  935,  936,\n",
      "         937,  938,  939,  940,  941,  942,  943,  944,  945,  946,  947,  948,\n",
      "         949,  950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
      "         961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,  972,\n",
      "         973,  974,  975,  976,  977,  978,  979,  980,  981,  982,  983,  984,\n",
      "         985,  986,  987,  988,  989,  990,  991,  992,  993,  994,  995,  996,\n",
      "         997,  998,  999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008,\n",
      "        1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020,\n",
      "        1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032,\n",
      "        1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044,\n",
      "        1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056,\n",
      "        1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065])\n"
     ]
    }
   ],
   "source": [
    "print(fts,\n",
    "      lbls,\n",
    "      G,\n",
    "      idx_train,\n",
    "      idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4857735",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_class = int(lbls.max()) + 1\n",
    "print(n_class)\n",
    "model_ft = HGNN(in_ch=fts.shape[1], # 特征矩阵列数\n",
    "                    n_class=n_class,\n",
    "                    n_hid=128,\n",
    "                    dropout=0.5)\n",
    "optimizer = optim.Adam(model_ft.parameters(), lr=0.001,\n",
    "                           weight_decay=0.0005) # adam优化\n",
    "    # optimizer = optim.SGD(model_ft.parameters(), lr=0.01, weight_decay=cfg['weight_decay)\n",
    "schedular = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                               milestones=[100],\n",
    "                                               gamma=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss() # 分类预测任务中使用了交叉熵损失函数\n",
    "outputs = model_ft(fts, G)\n",
    "print(outputs)\n",
    "loss = criterion(outputs[idx_train], lbls[idx_train]) # 根据自定义的criterion计算训练误差\n",
    "_, preds = torch.max(outputs, 1)# torch.max(input,dim) 在行上返回每行的最大值，并返回索引\n",
    "print(preds)\n",
    "\n",
    "MSE = torch.nn.MSELoss(reduction = 'mean')\n",
    "loss = MSE(preds.float(),lbls.float())\n",
    "print(loss)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6651cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, print_freq=500):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % print_freq == 0:\n",
    "            print('-' * 10)\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            idx = idx_train if phase == 'train' else idx_test # 选择相应数据集\n",
    "\n",
    "            # Iterate over data.\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(fts, G)\n",
    "                loss = criterion(outputs[idx], lbls[idx]) # 根据自定义的criterion计算训练误差\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase 在训练阶段进行反向传播和优化\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * fts.size(0)\n",
    "            running_corrects += torch.sum(preds[idx] == lbls.data[idx])\n",
    "\n",
    "            epoch_loss = running_loss / len(idx)\n",
    "            epoch_acc = running_corrects.double() / len(idx)\n",
    "\n",
    "            if epoch % print_freq == 0:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model 存储预测效果最好时候的model\n",
    "            # deep copy 复制完整对象并且存储在独立的内存地址\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if epoch % print_freq == 0:\n",
    "            print(f'Best val Acc: {best_acc:4f}')\n",
    "            print('-' * 20)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts) # 读出最好的model并且返回\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cc9141bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main():\n",
    " \n",
    "    n_class = int(lbls.max()) + 1\n",
    "    # 定义模型，此处分类模型被耦合在HGNN函数中，需要去HGNN那里改预测任务\n",
    "    model_ft = HGNN(in_ch=fts.shape[1], # 特征矩阵列数\n",
    "                    n_class=n_class,\n",
    "                    n_hid=128,\n",
    "                    dropout=0.1)\n",
    "    model_ft = model_ft.to(device) # 将模型转移到设备上\n",
    "\n",
    "    optimizer = optim.Adam(model_ft.parameters(), lr=0.001,\n",
    "                           weight_decay=0.0005) # adam优化\n",
    "    # optimizer = optim.SGD(model_ft.parameters(), lr=0.01, weight_decay=cfg['weight_decay)\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                               milestones=[100],\n",
    "                                               gamma=0.9)\n",
    "    criterion = torch.nn.CrossEntropyLoss() # 分类预测任务中使用了交叉熵损失函数\n",
    "\n",
    "    model_ft = train_model(model_ft, criterion, optimizer, schedular,1000, print_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "59d84133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/999\n",
      "train Loss: 1.7092 Acc: 0.0445\n",
      "val Loss: 6.7558 Acc: 0.0188\n",
      "Best val Acc: 0.018779\n",
      "--------------------\n",
      "----------\n",
      "Epoch 100/999\n",
      "train Loss: 1.0054 Acc: 0.7644\n",
      "val Loss: 3.2953 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "----------\n",
      "Epoch 200/999\n",
      "train Loss: 0.9896 Acc: 0.7644\n",
      "val Loss: 3.2022 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "----------\n",
      "Epoch 300/999\n",
      "train Loss: 0.9847 Acc: 0.7644\n",
      "val Loss: 3.1948 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "----------\n",
      "Epoch 400/999\n",
      "train Loss: 0.9809 Acc: 0.7644\n",
      "val Loss: 3.2040 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "----------\n",
      "Epoch 500/999\n",
      "train Loss: 0.9791 Acc: 0.7644\n",
      "val Loss: 3.1971 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "----------\n",
      "Epoch 600/999\n",
      "train Loss: 0.9755 Acc: 0.7644\n",
      "val Loss: 3.1877 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "----------\n",
      "Epoch 700/999\n",
      "train Loss: 0.9707 Acc: 0.7644\n",
      "val Loss: 3.1821 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "----------\n",
      "Epoch 800/999\n",
      "train Loss: 0.9665 Acc: 0.7644\n",
      "val Loss: 3.1843 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "----------\n",
      "Epoch 900/999\n",
      "train Loss: 0.9624 Acc: 0.7644\n",
      "val Loss: 3.1761 Acc: 0.7934\n",
      "Best val Acc: 0.793427\n",
      "--------------------\n",
      "\n",
      "Training complete in 0m 24s\n",
      "Best val Acc: 0.793427\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# cfg = get_config('config/config.yaml')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f59815a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = int(lbls.max()) + 1\n",
    "model_try = HGNN(in_ch=fts.shape[1], # 特征矩阵列数\n",
    "                    n_class=n_class,\n",
    "                    n_hid=128,\n",
    "                    dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ba2882bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3880, -0.0823, -0.1223, -0.1648],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.4839, -0.0824, -0.1590, -0.2594],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.5691, -0.0444, -0.1979, -0.2617]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_try(fts,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3d0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
